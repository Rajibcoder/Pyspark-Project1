{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajibcoder/Pyspark-Project1/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "AvwNpdVtbhQe",
        "outputId": "93b0f29b-014b-4651-90ca-ac5ac1c071b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/orders.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-094e60ade411>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferschema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/orders.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/orders.csv."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"writerAPI\").getOrCreate()\n",
        "orderdf = spark.read.format(\"csv\")\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".option(\"path\",\"/content/orders.csv\").load()\n",
        "\n",
        "\n",
        "orderdf.write.format(\"json\").mode(\"overwrite\")\\\n",
        ".bucketBy(4,\"order_status\")\\\n",
        ".option(\"path\",\"/content/newfolder\").saveAsTable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdoPxE-7dlhv",
        "outputId": "5d3b9cce-fea2-478e-b7ff-d92d2befe3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|            Review|\n",
            "+------------------+\n",
            "|       Hello world|\n",
            "|PySpark is awesome|\n",
            "+------------------+\n",
            "\n",
            "+------------------+-----+\n",
            "|            Review|count|\n",
            "+------------------+-----+\n",
            "|       Hello world|    2|\n",
            "|PySpark is awesome|    3|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"writerAPI\").getOrCreate()\n",
        "def fun(a):\n",
        "  return len(a.split())\n",
        "df = spark.createDataFrame([(\"Hello world\",), (\"PySpark is awesome\",)], [\"Review\"])\n",
        "df.show()\n",
        "fun2 = udf(fun,IntegerType())\n",
        "df.withColumn(\"count\",fun2(\"Review\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FDQ30Wd6Drq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 101)]\n",
        "departments = [(101, \"HR\"), (102, \"IT\")]\n",
        "\n",
        "emp_schema = [\"ID\", \"Name\", \"DeptID\"]\n",
        "dept_schema = [\"DeptID\", \"DeptName\"]\n",
        "\n",
        "df_emp = spark.createDataFrame(employees, emp_schema)\n",
        "df_dept = spark.createDataFrame(departments, dept_schema)\n",
        "\n",
        "# Inner Join\n",
        "df_join = df_emp.join(df_dept, \"DeptID\", \"inner\")\n",
        "df_join.show()\n",
        "df_join.write.format(\"csv\").mode(\"overwrite\").option(\"header\",\"true\").option(\"path\",\"/content/sample_data/new_folder\").save()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIeRpmZLDrzE",
        "outputId": "2063a464-68e1-4b22-b169-b4ab290a87bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+-------+--------+\n",
            "|DeptID| ID|   Name|DeptName|\n",
            "+------+---+-------+--------+\n",
            "|   101|  1|  Alice|      HR|\n",
            "|   101|  3|Charlie|      HR|\n",
            "|   102|  2|    Bob|      IT|\n",
            "+------+---+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"writerAPI\").getOrCreate()\n",
        "data = [ (1, 101, '2024-12-15 09:30:00'), (2, 102, '2024-12-15 11:45:00'), (3, 103, '2024-12-15 12:10:00'), (4, 104, '2024-12-15 13:15:00'), (5, 105, '2024-12-15 14:20:00'), (6, 106, '2024-12-15 15:30:00'), (7, 107, '2024-12-15 16:40:00'), (8, 108, '2024-12-16 09:50:00'), (9, 109, '2024-12-16 10:30:00'), (10, 110, '2024-12-16 12:05:00'), (11, 111, '2024-12-16 13:50:00'), (12, 112, '2024-12-16 14:15:00'), (13, 113, '2024-12-16 15:30:00'), (14, 114, '2024-12-17 09:45:00'), (15, 115, '2024-12-17 11:20:00'), (16, 116, '2024-12-17 12:25:00'), (17, 117, '2024-12-17 13:30:00'), (18, 118, '2024-12-17 14:55:00'), (19, 119, '2024-12-17 15:10:00'), (20, 120, '2024-12-18 10:40:00') ]\n",
        "\n",
        "columns = [\"order_id\", \"product_id\", \"timestamp\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "df1 = df.withColumn(\"hour\",when((hour(\"timestamp\")>6) & (hour(\"timestamp\")<12),\"Morning\")\n",
        ".when((hour(\"timestamp\")>=12) & (hour(\"timestamp\")<15),\"Early Afternoon\")\n",
        ".otherwise(\"Late Afternoon\")).withColumn(\"DAY\",date_format(\"timestamp\",\"EEEE\"))\n",
        "\n",
        "df2 = df1.groupBy(\"DAY\",\"hour\").count()\n",
        "\n",
        "df3 = df2.withColumn(\"Rank\",rank().over(Window.partitionBy(\"hour\").orderBy(desc(\"count\"))))\n",
        "df3.show()\n",
        "df3.filter(df3.Rank<=2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZvOBAyeYTkY",
        "outputId": "4a3b78bb-bf6b-44e4-d1f3-fd0849d99d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+-----+----+\n",
            "|      DAY|           hour|count|Rank|\n",
            "+---------+---------------+-----+----+\n",
            "|   Sunday|Early Afternoon|    3|   1|\n",
            "|   Monday|Early Afternoon|    3|   1|\n",
            "|  Tuesday|Early Afternoon|    3|   1|\n",
            "|   Sunday| Late Afternoon|    2|   1|\n",
            "|   Monday| Late Afternoon|    1|   2|\n",
            "|  Tuesday| Late Afternoon|    1|   2|\n",
            "|   Sunday|        Morning|    2|   1|\n",
            "|   Monday|        Morning|    2|   1|\n",
            "|  Tuesday|        Morning|    2|   1|\n",
            "|Wednesday|        Morning|    1|   4|\n",
            "+---------+---------------+-----+----+\n",
            "\n",
            "+-------+---------------+-----+----+\n",
            "|    DAY|           hour|count|Rank|\n",
            "+-------+---------------+-----+----+\n",
            "| Sunday|Early Afternoon|    3|   1|\n",
            "| Monday|Early Afternoon|    3|   1|\n",
            "|Tuesday|Early Afternoon|    3|   1|\n",
            "| Sunday| Late Afternoon|    2|   1|\n",
            "| Monday| Late Afternoon|    1|   2|\n",
            "|Tuesday| Late Afternoon|    1|   2|\n",
            "| Sunday|        Morning|    2|   1|\n",
            "| Monday|        Morning|    2|   1|\n",
            "|Tuesday|        Morning|    2|   1|\n",
            "+-------+---------------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.appName(\"writerAPI\").getOrCreate()\n",
        "data = [ (\"Store A\", \"2024-01\", 800), (\"Store A\", \"2024-02\", 1200), (\"Store A\", \"2024-03\", 900), (\"Store B\", \"2024-01\", 1500), (\"Store B\", \"2024-02\", 1600), (\"Store B\", \"2024-03\", 1400), (\"Store C\", \"2024-01\", 700), (\"Store C\", \"2024-02\", 1000), (\"Store C\", \"2024-03\", 800) ]\n",
        "df = spark.createDataFrame(data, [\"Store\", \"Month\", \"Sales\"])\n",
        "df.show()\n",
        "lessthen_1000 = df.filter(col(\"Sales\")<1000)\n",
        "df1 = df.groupBy(col(\"Store\")).agg(sum(\"Sales\").alias(\"Total Sales\")).orderBy(desc(\"Total Sales\"))\n",
        "df1.show()\n",
        "df.printSchema()\n",
        "df.withColumn(\"cumulative sales\",sum(\"Sales\").over(Window.partitionBy(\"Store\").orderBy(\"Month\"))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0eD_2gwyHAY",
        "outputId": "6dc69642-c259-4af2-b2f5-7dcd19cef990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|  Store|  Month|Sales|\n",
            "+-------+-------+-----+\n",
            "|Store A|2024-01|  800|\n",
            "|Store A|2024-02| 1200|\n",
            "|Store A|2024-03|  900|\n",
            "|Store B|2024-01| 1500|\n",
            "|Store B|2024-02| 1600|\n",
            "|Store B|2024-03| 1400|\n",
            "|Store C|2024-01|  700|\n",
            "|Store C|2024-02| 1000|\n",
            "|Store C|2024-03|  800|\n",
            "+-------+-------+-----+\n",
            "\n",
            "+-------+-----------+\n",
            "|  Store|Total Sales|\n",
            "+-------+-----------+\n",
            "|Store B|       4500|\n",
            "|Store A|       2900|\n",
            "|Store C|       2500|\n",
            "+-------+-----------+\n",
            "\n",
            "root\n",
            " |-- Store: string (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Sales: long (nullable = true)\n",
            "\n",
            "+-------+-------+-----+----------------+\n",
            "|  Store|  Month|Sales|cumulative sales|\n",
            "+-------+-------+-----+----------------+\n",
            "|Store A|2024-01|  800|             800|\n",
            "|Store A|2024-02| 1200|            2000|\n",
            "|Store A|2024-03|  900|            2900|\n",
            "|Store B|2024-01| 1500|            1500|\n",
            "|Store B|2024-02| 1600|            3100|\n",
            "|Store B|2024-03| 1400|            4500|\n",
            "|Store C|2024-01|  700|             700|\n",
            "|Store C|2024-02| 1000|            1700|\n",
            "|Store C|2024-03|  800|            2500|\n",
            "+-------+-------+-----+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"PivotTableExample\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 5000),\n",
        "    (2, \"Bob\", \"Sales\", 7000),\n",
        "    (3, \"Alice\", \"Marketing\", 4500),\n",
        "    (4, \"Bob\", \"Marketing\", 6500),\n",
        "    (5, \"Alice\", \"IT\", 8000),\n",
        "    (6, \"Bob\", \"IT\", 9000)\n",
        "]\n",
        "\n",
        "# Column Names\n",
        "columns = [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1 = df.groupBy(\"Employee\").pivot(\"Department\").sum(\"Salary\")\n",
        "df2 = df.groupBy(\"Department\").pivot(\"Employee\").sum(\"Salary\")\n",
        "df1.show()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50c-6a3lKqb",
        "outputId": "8494e165-955f-4ded-8f3f-dbde2c0292ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|   Alice|     Sales|  5000|\n",
            "|  2|     Bob|     Sales|  7000|\n",
            "|  3|   Alice| Marketing|  4500|\n",
            "|  4|     Bob| Marketing|  6500|\n",
            "|  5|   Alice|        IT|  8000|\n",
            "|  6|     Bob|        IT|  9000|\n",
            "+---+--------+----------+------+\n",
            "\n",
            "+--------+----+---------+-----+\n",
            "|Employee|  IT|Marketing|Sales|\n",
            "+--------+----+---------+-----+\n",
            "|     Bob|9000|     6500| 7000|\n",
            "|   Alice|8000|     4500| 5000|\n",
            "+--------+----+---------+-----+\n",
            "\n",
            "+----------+-----+----+\n",
            "|Department|Alice| Bob|\n",
            "+----------+-----+----+\n",
            "|     Sales| 5000|7000|\n",
            "| Marketing| 4500|6500|\n",
            "|        IT| 8000|9000|\n",
            "+----------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Z7lqr4ae_S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worker_data = [\n",
        "    \"1|John|Doe|5000|2023-01-01|Engineering\",\n",
        "    \"2|Jane|Smith|6000|2023-01-15|Marketing\",\n",
        "    \"3|Alice|Johnson|4500|2023-02-05|Engineering\"\n",
        "]\n",
        "data = [tuple(i.split(\"|\")) for i in worker_data]\n",
        "print(data)\n",
        "title_data = [\n",
        "    \"1|Engineer|2022-01-01\",\n",
        "    \"2|Manager|2022-01-01\",\n",
        "    \"3|Engineer|2022-01-01\"\n",
        "]\n",
        "\n",
        "title = [tuple(i.split(\"|\")) for i in title_data]\n",
        "\n",
        "worker_schema = [\"worker_id\", \"first_name\", \"last_name\", \"salary\", \"joining_date\", \"department\"]\n",
        "title_schema = [\"worker_ref_id\", \"worker_title\", \"affected_from\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data, worker_schema)\n",
        "df3 = spark.createDataFrame(title, title_schema)\n",
        "\n",
        "\n",
        "\n",
        "max_sal = df1.agg(max(\"salary\")).collect()[0][0]\n",
        "min_sal = df1.agg(min(\"salary\")).collect()[0][0]\n",
        "\n",
        "df2 = df1.withColumn(\"type\",when(df1.salary==max_sal,\"Maximum\").when(df1.salary==min_sal,\"Minimum\"))\n",
        "# df2 = df2.filter(col(\"type\").isNotNull())\n",
        "df2 = df2.dropna(subset=\"type\")\n",
        "\n",
        "df2.join(df3,df2.worker_id==df3.worker_ref_id,\"inner\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EpOqqoEJU8Q",
        "outputId": "9768f3dc-1170-4957-94d9-70e236324456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', 'John', 'Doe', '5000', '2023-01-01', 'Engineering'), ('2', 'Jane', 'Smith', '6000', '2023-01-15', 'Marketing'), ('3', 'Alice', 'Johnson', '4500', '2023-02-05', 'Engineering')]\n",
            "+---------+----------+---------+------+------------+-----------+-------+-------------+------------+-------------+\n",
            "|worker_id|first_name|last_name|salary|joining_date| department|   type|worker_ref_id|worker_title|affected_from|\n",
            "+---------+----------+---------+------+------------+-----------+-------+-------------+------------+-------------+\n",
            "|        2|      Jane|    Smith|  6000|  2023-01-15|  Marketing|Maximum|            2|     Manager|   2022-01-01|\n",
            "|        3|     Alice|  Johnson|  4500|  2023-02-05|Engineering|Minimum|            3|    Engineer|   2022-01-01|\n",
            "+---------+----------+---------+------+------------+-----------+-------+-------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (123, \"A\", \"AScore\", 30),\n",
        "    (123, \"A\", \"BScore\", 31),\n",
        "    (123, \"A\", \"CScore\", 32),\n",
        "    (124, \"B\", \"AScore\", 40),\n",
        "    (124, \"B\", \"BScore\", 41),\n",
        "    (124, \"B\", \"CScore\", 42)\n",
        "]\n",
        "df = spark.createDataFrame(data,[\"id\",\"name\",\"Subject\",\"score\"])\n",
        "df.groupBy(\"id\",\"name\").pivot(\"Subject\").sum(\"score\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joRpFMuHn5VJ",
        "outputId": "4244457c-438b-47e0-b080-a76384a30b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+------+------+\n",
            "| id|name|AScore|BScore|CScore|\n",
            "+---+----+------+------+------+\n",
            "|123|   A|    30|    31|    32|\n",
            "|124|   B|    40|    41|    42|\n",
            "+---+----+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (1, \"Laptop\", 1000, 5), (2, \"Mouse\", None, None), (3, \"Keyboard\", 50, 2), (4, \"Monitor\", 200, None), (5, None, 500, None), ]\n",
        "\n",
        "# Define schema and create DataFrame\n",
        "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "avg_price = df.agg(avg(\"price\")).collect()[0][0]\n",
        "df1 = df.fillna(avg_price,subset=\"price\")\n",
        "df1.dropna(subset=\"product\").fillna(1,subset=\"quantity\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QXbNG9Ct7U9",
        "outputId": "2619f070-c1fb-4f09-e0c6-b177b2a680a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----+--------+\n",
            "|product_id| product|price|quantity|\n",
            "+----------+--------+-----+--------+\n",
            "|         1|  Laptop| 1000|       5|\n",
            "|         2|   Mouse|  437|       1|\n",
            "|         3|Keyboard|   50|       2|\n",
            "|         4| Monitor|  200|       1|\n",
            "+----------+--------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (1, \"Laptop\", \"2024-08-01\"), (1, \"Mouse\", \"2024-08-05\"), (2, \"Keyboard\", \"2024-08-02\"), (2, \"Monitor\", \"2024-08-03\") ]\n",
        "\n",
        "columns = [\"customer_id\", \"product\", \"purchase_date\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "df = df.withColumn(\"date\",col(\"purchase_date\").cast(\"date\")).drop(\"purchase_date\")\n",
        "df.withColumn(\"Rank\",row_number().over(Window.partitionBy(\"customer_id\").orderBy(col(\"date\").desc()))).filter(col(\"Rank\")==1).drop(\"Rank\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCkBqSkSy1JG",
        "outputId": "c8405d64-eff7-4ac5-e2a3-d4e365072242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----------+\n",
            "|customer_id|product|      date|\n",
            "+-----------+-------+----------+\n",
            "|          1|  Mouse|2024-08-05|\n",
            "|          2|Monitor|2024-08-03|\n",
            "+-----------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Laptop\", 800), (\"Mouse\", 25), (\"Keyboard\", 150), (\"Monitor\", 300)]\n",
        "columns = [\"product\", \"price\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.withColumn(\"Type\",when(col(\"price\") < 100,\"low\").when((col(\"price\") >= 100) & (col(\"price\") < 500),\"Medium\").otherwise(\"High\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izh0MtoRBAj1",
        "outputId": "03b6a92f-82a8-478d-ddd2-9d02b74f9825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+------+\n",
            "| product|price|  Type|\n",
            "+--------+-----+------+\n",
            "|  Laptop|  800|  High|\n",
            "|   Mouse|   25|   low|\n",
            "|Keyboard|  150|Medium|\n",
            "| Monitor|  300|Medium|\n",
            "+--------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (\"U1\", \"2024-12-30 10:00:00\", \"LOGIN\"), (\"U1\", \"2024-12-30 10:05:00\", \"BROWSE\"), (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"), (\"U2\", \"2024-12-30 11:00:00\", \"LOGIN\"), (\"U2\", \"2024-12-30 11:15:00\", \"BROWSE\"), (\"U2\", \"2024-12-30 11:30:00\", \"LOGOUT\"), (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),(None, \"2024-12-30 12:00:00\", \"LOGIN\"),(\"U3\", None, \"LOGOUT\")]\n",
        "\n",
        "\n",
        "schema = StructType([ StructField(\"user_id\", StringType(), True), StructField(\"timestamp\", StringType(), True), StructField(\"activity_type\", StringType(), True) ])\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df = df.dropDuplicates().dropna(subset = [\"user_id\",\"timestamp\",\"activity_type\"])\n",
        "df = df.withColumn(\"activetime\",coalesce(lag(\"timestamp\").over(Window.partitionBy(\"user_id\").orderBy(col(\"timestamp\").desc())),col(\"timestamp\")))\n",
        "df.withColumn(\"final\",from_unixtime(unix_timestamp(\"activetime\")-unix_timestamp(\"timestamp\"),\"HH:mm:ss\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_TTMHJ9EXgK",
        "outputId": "d122d294-cb75-4a90-c631-ca2eb8bfad89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------+-------------------+--------+\n",
            "|user_id|          timestamp|activity_type|         activetime|   final|\n",
            "+-------+-------------------+-------------+-------------------+--------+\n",
            "|     U1|2024-12-30 10:20:00|       LOGOUT|2024-12-30 10:20:00|00:00:00|\n",
            "|     U1|2024-12-30 10:05:00|       BROWSE|2024-12-30 10:20:00|00:15:00|\n",
            "|     U1|2024-12-30 10:00:00|        LOGIN|2024-12-30 10:05:00|00:05:00|\n",
            "|     U2|2024-12-30 11:30:00|       LOGOUT|2024-12-30 11:30:00|00:00:00|\n",
            "|     U2|2024-12-30 11:15:00|       BROWSE|2024-12-30 11:30:00|00:15:00|\n",
            "|     U2|2024-12-30 11:00:00|        LOGIN|2024-12-30 11:15:00|00:15:00|\n",
            "+-------+-------------------+-------------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (\"A\", \"p1\", 100),(\"A\", \"p1\", 100), (\"A\", \"p2\", 200), (\"A\", \"p3\", 200), (\"B\", \"p4\", 300), (\"B\", \"p5\", 150), (\"B\", \"p6\", 150), (\"C\", \"p7\", 400), (\"C\", \"p8\", 300), (\"C\", \"p9\", 200), ]\n",
        "\n",
        "columns = [\"category\", \"product\", \"amount\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "# df.show()\n",
        "df = df.groupBy(\"category\", \"product\").agg(sum(\"amount\").alias(\"amount\")).orderBy(\"category\",\"product\")\n",
        "WindowSpec = Window.partitionBy(\"category\").orderBy(col(\"amount\").desc())\n",
        "df.withColumn(\"result\",dense_rank().over(WindowSpec)).filter(col(\"result\")<=3).drop(\"result\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRzbu1Ddh9KF",
        "outputId": "c826d622-4be3-4d70-b36f-f25179c9704c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+------+\n",
            "|category|product|amount|\n",
            "+--------+-------+------+\n",
            "|       A|     p1|   200|\n",
            "|       A|     p2|   200|\n",
            "|       A|     p3|   200|\n",
            "|       B|     p4|   300|\n",
            "|       B|     p5|   150|\n",
            "|       B|     p6|   150|\n",
            "|       C|     p7|   400|\n",
            "|       C|     p8|   300|\n",
            "|       C|     p9|   200|\n",
            "+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (\"HR\", \"2020\", 2500),\n",
        " (\"HR\", \"2021\", 3200),\n",
        " (\"HR\", \"2022\", 2800),\n",
        " (\"Engineering\", \"2020\", 5000),\n",
        " (\"Engineering\", \"2021\", 6000),\n",
        " (\"Engineering\", \"2022\", 5500),\n",
        " (\"Marketing\", \"2020\", 4000),\n",
        " (\"Marketing\", \"2021\", 3500),\n",
        " (\"Marketing\", \"2022\", 3300)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"Department\", \"Year\", \"Salary\"])\n",
        "df1 = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"avg\"),sum(\"Salary\").alias(\"total\"))\n",
        "df1 = df1.filter(col(\"avg\")>3000).orderBy(col(\"total\").desc())\n",
        "df1.show()\n",
        "\n",
        "# WindowSpec = Window.partitionBy(\"Department\")\n",
        "# df.withColumn(\"avg_sal\",avg(\"Salary\").over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLLwK8e4rWGw",
        "outputId": "570501c5-a151-448d-c782-472905283fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+-----+\n",
            "| Department|   avg|total|\n",
            "+-----------+------+-----+\n",
            "|Engineering|5500.0|16500|\n",
            "|  Marketing|3600.0|10800|\n",
            "+-----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (1, 101, 500.0, \"2024-01-01\"), (2, 102, 200.0, \"2024-01-02\"),\n",
        "(3, 101, 300.0, \"2024-01-03\"), (4, 103, 100.0, \"2024-01-04\"),\n",
        "(5, 102, 400.0, \"2024-01-05\"), (6, 103, 600.0, \"2024-01-06\"),\n",
        "(7, 101, 200.0, \"2024-01-07\"), ]\n",
        "\n",
        "columns = [\"transaction_id\", \"user_id\", \"transaction_amount\", \"transaction_date\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "df = df.groupBy(\"user_id\").agg(sum(\"transaction_amount\").alias(\"total_Amount\"),max(\"transaction_date\").alias(\"Last_date\")).limit(3)\n",
        "df.sort(col(\"total_Amount\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT5YFTjD-oin",
        "outputId": "217edddd-2531-420d-ecd0-a357b7a61601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+\n",
            "|user_id|total_Amount| Last_date|\n",
            "+-------+------------+----------+\n",
            "|    101|      1000.0|2024-01-07|\n",
            "|    103|       700.0|2024-01-06|\n",
            "|    102|       600.0|2024-01-05|\n",
            "+-------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (1, 101, 500.0, \"2024-01-01\"),\n",
        " (2, 102, 200.0, \"2024-01-02\"),\n",
        " (3, 101, 300.0, \"2024-01-03\"),\n",
        " (4, 103, 100.0, \"2024-01-04\"),\n",
        " (5, 102, 400.0, \"2024-01-05\"),\n",
        " (6, 103, 600.0, \"2024-01-06\"),\n",
        " (7, 101, 200.0, \"2024-01-07\"),\n",
        "]\n",
        "columns = [\"transaction_id\", \"user_id\", \"transaction_amount\", \"transaction_date\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.groupBy(\"user_id\").agg(count_distinct(\"transaction_date\").alias(\"count_distnict_date\"),avg(\"transaction_amount\").alias(\"avg_amount\")).filter(col(\"count_distnict_date\")>=3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDjqfrgCCfd",
        "outputId": "b01a388b-de1e-485d-f3e3-e10b84267448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------+------------------+----------------+\n",
            "|transaction_id|user_id|transaction_amount|transaction_date|\n",
            "+--------------+-------+------------------+----------------+\n",
            "|             1|    101|             500.0|      2024-01-01|\n",
            "|             2|    102|             200.0|      2024-01-02|\n",
            "|             3|    101|             300.0|      2024-01-03|\n",
            "|             4|    103|             100.0|      2024-01-04|\n",
            "|             5|    102|             400.0|      2024-01-05|\n",
            "|             6|    103|             600.0|      2024-01-06|\n",
            "|             7|    101|             200.0|      2024-01-07|\n",
            "+--------------+-------+------------------+----------------+\n",
            "\n",
            "+-------+-------------------+-----------------+\n",
            "|user_id|count_distnict_date|       avg_amount|\n",
            "+-------+-------------------+-----------------+\n",
            "|    101|                  3|333.3333333333333|\n",
            "+-------+-------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (1, 101, 500.0, \"2024-01-01\"),\n",
        " (2, 102, 200.0, \"2024-01-02\"),\n",
        " (3, 101, 300.0, \"2024-01-03\"),\n",
        " (4, 103, 100.0, \"2024-01-04\"),\n",
        " (5, 102, 400.0, \"2024-01-05\"),\n",
        " (6, 103, 600.0, \"2024-01-06\"),\n",
        " (7, 101, 200.0, \"2024-01-07\"),\n",
        "]\n",
        "columns = [\"transaction_id\", \"user_id\", \"transaction_amount\", \"transaction_date\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df = df.withColumn(\"transaction_date\",col(\"transaction_date\").cast(\"date\"))\n",
        "WindowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"transaction_date\").desc())\n",
        "df = df.withColumn(\"lag\",lag(\"transaction_date\").over(WindowSpec))\n",
        "df = df.withColumn(\"gap\",datediff(\"lag\",\"transaction_date\"))\n",
        "df = df.groupBy(\"user_id\").agg(avg(\"gap\").alias(\"avg_gap\"))\n",
        "df.show()\n",
        "df.orderBy(col(\"avg_gap\").desc()).limit(1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uASKtfm2PUD1",
        "outputId": "b0dcb504-92f2-46a5-83df-4a17a25e20b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|user_id|avg_gap|\n",
            "+-------+-------+\n",
            "|    101|    3.0|\n",
            "|    102|    3.0|\n",
            "|    103|    2.0|\n",
            "+-------+-------+\n",
            "\n",
            "+-------+-------+\n",
            "|user_id|avg_gap|\n",
            "+-------+-------+\n",
            "|    101|    3.0|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (\"Alice\", \"Math\", \"Semester1\", 85),\n",
        " (\"Alice\", \"Math\", \"Semester2\", 90),\n",
        " (\"Alice\", \"English\", \"Semester1\", 78),\n",
        " (\"Alice\", \"English\", \"Semester2\", 82),\n",
        " (\"Bob\", \"Math\", \"Semester1\", 65),\n",
        " (\"Bob\", \"Math\", \"Semester2\", 70),\n",
        " (\"Bob\", \"English\", \"Semester1\", 60),\n",
        " (\"Bob\", \"English\", \"Semester2\", 65),\n",
        " (\"Charlie\", \"Math\", \"Semester1\", 95),\n",
        " (\"Charlie\", \"Math\", \"Semester2\", 98),\n",
        " (\"Charlie\", \"English\", \"Semester1\", 88),\n",
        " (\"Charlie\", \"English\", \"Semester2\", 90),\n",
        " (\"David\", \"Math\", \"Semester1\", 78),\n",
        " (\"David\", \"Math\", \"Semester2\", 80),\n",
        " (\"David\", \"English\", \"Semester1\", 75),\n",
        " (\"David\", \"English\", \"Semester2\", 72),\n",
        " (\"Eve\", \"Math\", \"Semester1\", 88),\n",
        " (\"Eve\", \"Math\", \"Semester2\", 85),\n",
        " (\"Eve\", \"English\", \"Semester1\", 80),\n",
        " (\"Eve\", \"English\", \"Semester2\", 83)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"Student\", \"Subject\", \"Semester\", \"Grade\"])\n",
        "df.show()\n",
        "df.groupBy(\"Student\").agg(avg(\"Grade\").alias(\"avg_grade\"))\n",
        "\n",
        "# WindowSpec = Window.partitionBy(\"Student\")\n",
        "# df.withColumn(\"avg_grade\",avg(\"Grade\").over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYGUhuCXU0r0",
        "outputId": "9ca996bb-9053-4e58-adc4-1ea86d1dd3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+---------+-----+\n",
            "|Student|Subject| Semester|Grade|\n",
            "+-------+-------+---------+-----+\n",
            "|  Alice|   Math|Semester1|   85|\n",
            "|  Alice|   Math|Semester2|   90|\n",
            "|  Alice|English|Semester1|   78|\n",
            "|  Alice|English|Semester2|   82|\n",
            "|    Bob|   Math|Semester1|   65|\n",
            "|    Bob|   Math|Semester2|   70|\n",
            "|    Bob|English|Semester1|   60|\n",
            "|    Bob|English|Semester2|   65|\n",
            "|Charlie|   Math|Semester1|   95|\n",
            "|Charlie|   Math|Semester2|   98|\n",
            "|Charlie|English|Semester1|   88|\n",
            "|Charlie|English|Semester2|   90|\n",
            "|  David|   Math|Semester1|   78|\n",
            "|  David|   Math|Semester2|   80|\n",
            "|  David|English|Semester1|   75|\n",
            "|  David|English|Semester2|   72|\n",
            "|    Eve|   Math|Semester1|   88|\n",
            "|    Eve|   Math|Semester2|   85|\n",
            "|    Eve|English|Semester1|   80|\n",
            "|    Eve|English|Semester2|   83|\n",
            "+-------+-------+---------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Student: string, avg_grade: double]"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "products_data = [ (1, \"Laptop\", 101), (2, \"Smartphone\", 102), (3, \"Tablet\", 101) ]\n",
        "\n",
        "products_schema = [\"product_id\", \"product_name\", \"category_id\"]\n",
        "\n",
        "products_df = spark.createDataFrame(products_data, schema=products_schema)\n",
        "products_df.show()\n",
        "\n",
        "categories_data = [ (101, \"Electronics\"), (102, \"Mobile\"), (103, \"Home Appliance\") ]\n",
        "\n",
        "categories_schema = [\"cat_id\", \"category_name\"]\n",
        "\n",
        "categories_df = spark.createDataFrame(categories_data,categories_schema)\n",
        "categories_df.show()\n",
        "\n",
        "products_df.join(categories_df,products_df.category_id==categories_df.cat_id,\"inner\").drop(\"category_id\",\"cat_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZGtT4ffjX6s",
        "outputId": "ae7438ec-aacc-428c-cd26-621a212bdb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+\n",
            "|product_id|product_name|category_id|\n",
            "+----------+------------+-----------+\n",
            "|         1|      Laptop|        101|\n",
            "|         2|  Smartphone|        102|\n",
            "|         3|      Tablet|        101|\n",
            "+----------+------------+-----------+\n",
            "\n",
            "+------+--------------+\n",
            "|cat_id| category_name|\n",
            "+------+--------------+\n",
            "|   101|   Electronics|\n",
            "|   102|        Mobile|\n",
            "|   103|Home Appliance|\n",
            "+------+--------------+\n",
            "\n",
            "+----------+------------+-------------+\n",
            "|product_id|product_name|category_name|\n",
            "+----------+------------+-------------+\n",
            "|         1|      Laptop|  Electronics|\n",
            "|         3|      Tablet|  Electronics|\n",
            "|         2|  Smartphone|       Mobile|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"writerAPI\").getOrCreate()"
      ],
      "metadata": {
        "id": "zje0aiXYgeOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data = [\n",
        " {\"transaction_id\": 1, \"customer_id\": 101, \"amount\": 150, \"date\": \"2025-01-01\"},\n",
        " {\"transaction_id\": 2, \"customer_id\": 102, \"amount\": 90, \"date\": \"2025-01-02\"},\n",
        " {\"transaction_id\": 3, \"customer_id\": 103, \"amount\": 200, \"date\": \"2025-01-03\"},\n",
        " {\"transaction_id\": 4, \"customer_id\": 104, \"amount\": 50, \"date\": \"2025-01-04\"},\n",
        " {\"transaction_id\": 5, \"customer_id\": 105, \"amount\": 120, \"date\": \"2025-01-05\"}\n",
        "]\n",
        "df = spark.createDataFrame(transaction_data)\n",
        "df.printSchema()\n",
        "df.filter(col(\"amount\")>100).show()\n",
        "df.withColumn(\"discounted_amount\",(col(\"amount\")-(col(\"amount\")*0.10))).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngvTDRm-Wye9",
        "outputId": "286d75c8-c244-457d-d5e7-311e55dcfea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- amount: long (nullable = true)\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- transaction_id: long (nullable = true)\n",
            "\n",
            "+------+-----------+----------+--------------+\n",
            "|amount|customer_id|      date|transaction_id|\n",
            "+------+-----------+----------+--------------+\n",
            "|   150|        101|2025-01-01|             1|\n",
            "|   200|        103|2025-01-03|             3|\n",
            "|   120|        105|2025-01-05|             5|\n",
            "+------+-----------+----------+--------------+\n",
            "\n",
            "+------+-----------+----------+--------------+-----------------+\n",
            "|amount|customer_id|      date|transaction_id|discounted_amount|\n",
            "+------+-----------+----------+--------------+-----------------+\n",
            "|   150|        101|2025-01-01|             1|            135.0|\n",
            "|    90|        102|2025-01-02|             2|             81.0|\n",
            "|   200|        103|2025-01-03|             3|            180.0|\n",
            "|    50|        104|2025-01-04|             4|             45.0|\n",
            "|   120|        105|2025-01-05|             5|            108.0|\n",
            "+------+-----------+----------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (1, \"Laptop\", 1000, 5),\n",
        " (2, \"Mouse\", None, None),\n",
        " (3, \"Keyboard\", 50, 2),\n",
        " (4, \"Monitor\", 200, None),\n",
        " (5, None, 500, None),\n",
        "]\n",
        "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "a = df.agg(median(\"price\")).collect()[0][0]\n",
        "df=df.fillna(a,subset=\"price\")\n",
        "# df.when(\"price\">300,df.fillna(\"Unknown\",subset=\"product\"))\n",
        "df = df.withColumn(\"product\",when(col(\"product\").isNull() & (col(\"price\") > 300), \"Unknown\").otherwise(col(\"product\")))\n",
        "df=df.dropna(subset=\"product\")\n",
        "b = df.agg(round(avg(\"quantity\"))).collect()[0][0]\n",
        "\n",
        "df=df.fillna(b,subset=\"quantity\")\n",
        "df=df.withColumn(\"total_value\",col(\"price\")*col(\"quantity\")).drop(\"product_id\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi2EojjpZOVV",
        "outputId": "eb726f4f-c114-4718-d4db-b1bae7c7c1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------+-----------+\n",
            "| product|price|quantity|total_value|\n",
            "+--------+-----+--------+-----------+\n",
            "|  Laptop| 1000|       5|       5000|\n",
            "|   Mouse|  350|       4|       1400|\n",
            "|Keyboard|   50|       2|        100|\n",
            "| Monitor|  200|       4|        800|\n",
            "| Unknown|  500|       4|       2000|\n",
            "+--------+-----+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (\"A\", \"North\", 100), (\"B\", \"East\", 200), (\"A\", \"East\", 150), (\"C\", \"North\", 300), (\"B\", \"South\", 400), (\"C\", \"East\", 250) ]\n",
        "columns = [\"Product\", \"Region\", \"Sales\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df=df.groupBy(\"Product\").pivot(\"Region\").agg(sum(\"Sales\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwYZi2Y5gh_Z",
        "outputId": "049cc03c-8d9f-4279-811c-0d9f84ef62c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-----+\n",
            "|Product|East|North|South|\n",
            "+-------+----+-----+-----+\n",
            "|      B| 200| NULL|  400|\n",
            "|      C| 250|  300| NULL|\n",
            "|      A| 150|  100| NULL|\n",
            "+-------+----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBkUx0ezRSP5ErSVHC8rUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}